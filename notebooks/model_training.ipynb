{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585804b6",
   "metadata": {},
   "source": [
    "## Milestone II — Build Catalog, Train Lightweight Models, Export Bundle\n",
    "\n",
    "Outputs created by this notebook\n",
    "\n",
    "- data/site_items.json – item catalog for browse/search in the Flask UI\n",
    "\n",
    "- model/ensemble.pkl – equal-weight ensemble (Count+LR, TF-IDF+LR, TF-IDF→SVD+LR)\n",
    "\n",
    "- model/manifest.json – small human/debug summary for the web app\n",
    "\n",
    "\n",
    "\n",
    "Why these?\n",
    "\n",
    "- Fast to train & load, zero external downloads, portable across machines.\n",
    "\n",
    "- Meets Milestone II requirement: auto “Recommended” label for new reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b2470e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/mac/Desktop/dem-web\n",
      "DATA_CSV    : /Users/mac/Desktop/dem-web/data/assignment3_II.csv\n",
      "DATA_DIR    : /Users/mac/Desktop/dem-web/data\n",
      "MODEL_DIR   : /Users/mac/Desktop/dem-web/model\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1. Imports, paths, reproducibility ===\n",
    "import os, re, json, warnings, random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Resolve project paths so this notebook works from project root or /notebooks\n",
    "NB_DIR = Path.cwd().resolve()\n",
    "CANDIDATES = [\n",
    "    NB_DIR / \"data\" / \"assignment3_II.csv\",\n",
    "    NB_DIR.parent / \"data\" / \"assignment3_II.csv\",\n",
    "    NB_DIR.parents[1] / \"data\" / \"assignment3_II.csv\",\n",
    "]\n",
    "DATA_CSV = next((p for p in CANDIDATES if p.exists()), None)\n",
    "if DATA_CSV is None:\n",
    "    raise FileNotFoundError(\"Place 'assignment3_II.csv' under project_root/data/\")\n",
    "\n",
    "PROJECT_ROOT = DATA_CSV.parent.parent\n",
    "DATA_DIR  = PROJECT_ROOT / \"data\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"model\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CATALOG_JSON  = DATA_DIR  / \"site_items.json\"\n",
    "BUNDLE_PKL    = MODEL_DIR / \"ensemble.pkl\"\n",
    "MANIFEST_JSON = MODEL_DIR / \"manifest.json\"\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_CSV    :\", DATA_CSV)\n",
    "print(\"DATA_DIR    :\", DATA_DIR)\n",
    "print(\"MODEL_DIR   :\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391f10c",
   "metadata": {},
   "source": [
    "## Load CSV & normalize columns\n",
    "\n",
    "We keep both display fields (Clothes Title/Description) and the original review fields for modeling.\n",
    "\n",
    "- Fill missing strings with \"\", coerce numeric columns safely.\n",
    "\n",
    "- Accept either Recommended IND or Recommended as the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97e2b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 19662\n",
      "Columns: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'Clothes Title', 'Clothes Description']\n",
      "Label column: Recommended IND | Positives: 16087\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2. Load CSV & normalize columns ===\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "# Add display fields if missing (fallbacks to legacy)\n",
    "if \"Clothes Title\" not in df.columns:\n",
    "    df[\"Clothes Title\"] = df.get(\"Title\", \"\").fillna(\"\")\n",
    "if \"Clothes Description\" not in df.columns:\n",
    "    df[\"Clothes Description\"] = df.get(\"Review Text\", \"\").fillna(\"\")\n",
    "\n",
    "# Ensure these exist\n",
    "need_cols = [\n",
    "    \"Clothing ID\", \"Clothes Title\", \"Clothes Description\",\n",
    "    \"Rating\", \"Division Name\", \"Department Name\", \"Class Name\",\n",
    "    \"Review Text\", \"Title\"\n",
    "]\n",
    "for c in need_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = \"\"\n",
    "\n",
    "# Fill NaNs\n",
    "df = df.fillna({\n",
    "    \"Clothing ID\": 0,\n",
    "    \"Clothes Title\": \"\",\n",
    "    \"Clothes Description\": \"\",\n",
    "    \"Rating\": 0,\n",
    "    \"Division Name\": \"\",\n",
    "    \"Department Name\": \"\",\n",
    "    \"Class Name\": \"\",\n",
    "    \"Review Text\": \"\",\n",
    "    \"Title\": \"\",\n",
    "})\n",
    "\n",
    "def to_int_safe(x, default=0):\n",
    "    try: return int(x)\n",
    "    except: return default\n",
    "\n",
    "df[\"Clothing ID\"] = df[\"Clothing ID\"].apply(to_int_safe)\n",
    "df[\"Rating\"]      = df[\"Rating\"].apply(to_int_safe)\n",
    "\n",
    "# Determine label column\n",
    "label_col = None\n",
    "for cand in [\"Recommended IND\", \"Recommended\"]:\n",
    "    if cand in df.columns:\n",
    "        label_col = cand\n",
    "        break\n",
    "if label_col is None:\n",
    "    raise ValueError(\"No label column found. Expect 'Recommended IND' or 'Recommended'.\")\n",
    "\n",
    "# Clean label & drop missing\n",
    "df[label_col] = pd.to_numeric(df[label_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "df = df[(df[label_col] == 0) | (df[label_col] == 1)].copy()\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"Label column:\", label_col, \"| Positives:\", int(df[label_col].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b824e827",
   "metadata": {},
   "source": [
    "## Plural-aware normalizer (for keyword search)\n",
    "\n",
    "We want “dress” and “dresses” to match the same items without heavy stemmers.\n",
    "\n",
    "- Lowercase, strip punctuation.\n",
    "\n",
    "- Rule-of-thumb plural reducer (common English endings).\n",
    "\n",
    "- Keep numbers/letters joined for simple matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2635c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3. Lightweight plural-aware normalizer ===\n",
    "_word_re = re.compile(r\"[A-Za-z]+(?:[-'][A-Za-z]+)?\")\n",
    "\n",
    "def normalize_for_search(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    tokens = _word_re.findall(text)\n",
    "\n",
    "    def reduce_plural(w: str) -> str:\n",
    "        if len(w) <= 3: return w\n",
    "        # common endings\n",
    "        for suf, rep in [(\"ies\",\"y\"), (\"sses\",\"ss\"), (\"xes\",\"x\"), (\"zes\",\"z\")]:\n",
    "            if w.endswith(suf): return w[:-len(suf)] + rep\n",
    "        for suf in (\"s\",\"es\"):\n",
    "            if w.endswith(suf) and not w.endswith(\"ss\"):\n",
    "                return w[:-len(suf)]\n",
    "        return w\n",
    "\n",
    "    return \" \".join(reduce_plural(t) for t in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df11dce",
   "metadata": {},
   "source": [
    "## Build search catalog for Milestone II UI\n",
    "\n",
    "We write a lightweight catalog data/site_items.json the Flask app can load to render previews / details:\n",
    "\n",
    "- id, clothes_title, clothes_desc, rating, division, department, class\n",
    "\n",
    "- preview (short teaser)\n",
    "\n",
    "- search_text: normalized string used by the keyword search (plural-aware reducer)\n",
    "\n",
    "\n",
    "\n",
    "If Clothes Title / Clothes Description are missing, we fall back to Title / first 120 chars of Review Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c0f3095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote catalog: /Users/mac/Desktop/dem-web/data/site_items.json | items: 1095\n",
      "Search normalizer test: dress dress → expect same token twice\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4. Build & save catalog ===\n",
    "def pick_first_per_id(frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    # If multiple rows per Clothing ID, pick the longest review text row as the representative\n",
    "    frame = frame.copy()\n",
    "    frame[\"__len\"] = frame[\"Review Text\"].str.len().fillna(0)\n",
    "    rep = (\n",
    "        frame.sort_values([\"Clothing ID\",\"__len\"], ascending=[True, False])\n",
    "             .drop_duplicates(subset=[\"Clothing ID\"], keep=\"first\")\n",
    "             .drop(columns=\"__len\")\n",
    "    )\n",
    "    return rep\n",
    "\n",
    "rep = pick_first_per_id(df)\n",
    "\n",
    "def mk_preview(desc: str) -> str:\n",
    "    desc = (desc or \"\").strip()\n",
    "    if len(desc) > 160:\n",
    "        return desc[:157] + \"...\"\n",
    "    return desc\n",
    "\n",
    "items = []\n",
    "for _, r in rep.iterrows():\n",
    "    cid   = int(r.get(\"Clothing ID\", 0))\n",
    "    title = (r.get(\"Clothes Title\") or r.get(\"Title\") or \"\").strip()\n",
    "    desc  = (r.get(\"Clothes Description\") or r.get(\"Review Text\") or \"\").strip()\n",
    "    rating = int(r.get(\"Rating\", 0))\n",
    "    div   = (r.get(\"Division Name\") or \"\").strip()\n",
    "    dept  = (r.get(\"Department Name\") or \"\").strip()\n",
    "    cls   = (r.get(\"Class Name\") or \"\").strip()\n",
    "\n",
    "    search_text = normalize_for_search(\" \".join([\n",
    "        title, desc, div, dept, cls\n",
    "    ]))\n",
    "\n",
    "    items.append({\n",
    "        \"id\": cid,\n",
    "        \"clothes_title\": title,\n",
    "        \"clothes_desc\": desc,\n",
    "        \"rating\": rating,\n",
    "        \"division\": div,\n",
    "        \"department\": dept,\n",
    "        \"class\": cls,\n",
    "        \"preview\": mk_preview(desc if desc else (r.get(\"Review Text\") or \"\")[:120]),\n",
    "        \"search_text\": search_text\n",
    "    })\n",
    "\n",
    "with open(CATALOG_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote catalog: {CATALOG_JSON} | items: {len(items)}\")\n",
    "print(\"Search normalizer test:\", normalize_for_search(\"dress / dresses\"), \"→ expect same token twice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7dadd5",
   "metadata": {},
   "source": [
    "## Modeling corpus & label\n",
    "\n",
    "We will:\n",
    "\n",
    "- Use Review Text as the core signal (best generalization).\n",
    "\n",
    "- Optionally also export a Title+Review model variant if needed later.\n",
    "\n",
    "- Drop completely empty reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4b0b4dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 19662 | positives: 16087\n",
      "Example: I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5. Build corpus & labels ===\n",
    "df[\"Review Text\"] = df[\"Review Text\"].fillna(\"\").astype(str)\n",
    "df[\"Title\"] = df[\"Title\"].fillna(\"\").astype(str)\n",
    "\n",
    "mask_nonempty = df[\"Review Text\"].str.strip().str.len() > 0\n",
    "dfe = df[mask_nonempty].copy()\n",
    "\n",
    "corpus_review = dfe[\"Review Text\"].tolist()\n",
    "corpus_both   = (dfe[\"Title\"].str.cat(dfe[\"Review Text\"], sep=\" . \").tolist())\n",
    "y             = dfe[label_col].astype(int).to_numpy()\n",
    "\n",
    "print(\"Samples:\", len(y), \"| positives:\", int(y.sum()))\n",
    "print(\"Example:\", corpus_review[0][:120] if len(corpus_review)>0 else \"(empty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f941d",
   "metadata": {},
   "source": [
    "## Define lightweight pipelines\n",
    "\n",
    "We use three compact models:\n",
    "\n",
    "1) Count + LogisticRegression\n",
    "\n",
    "2) TF-IDF + LogisticRegression\n",
    "\n",
    "3) TF-IDF → TruncatedSVD + Standardize + LogisticRegression (LSA style)\n",
    "\n",
    "\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Automatic feature limits (max_features) to avoid memory spikes.\n",
    "\n",
    "- Dynamic min_df to avoid “no terms remain” errors on small subsets.\n",
    "\n",
    "- Safe SVD dimension computed from a probe TF-IDF fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b69b59ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6. Pipelines (with safe parameters for small/large data) ===\n",
    "def choose_min_df(n_docs: int) -> int:\n",
    "    # Keep at least a few terms EVEN in very small CV folds\n",
    "    if n_docs >= 5000: return 3\n",
    "    if n_docs >= 1000: return 2\n",
    "    return 1\n",
    "\n",
    "def make_count_lr(n_docs: int, max_features=30000) -> Pipeline:\n",
    "    mindf = choose_min_df(n_docs)\n",
    "    return Pipeline([\n",
    "        (\"vec\", CountVectorizer(max_features=max_features, min_df=mindf, ngram_range=(1,2))),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000, n_jobs=None))\n",
    "    ])\n",
    "\n",
    "def make_tfidf_lr(n_docs: int, max_features=30000) -> Pipeline:\n",
    "    mindf = choose_min_df(n_docs)\n",
    "    return Pipeline([\n",
    "        (\"vec\", TfidfVectorizer(max_features=max_features, min_df=mindf, ngram_range=(1,2))),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000, n_jobs=None))\n",
    "    ])\n",
    "\n",
    "def make_tfidf_svd_lr(n_docs: int, svd_dim: int = None, max_features=30000) -> Pipeline:\n",
    "    mindf = choose_min_df(n_docs)\n",
    "    # Probe TF-IDF to pick a safe SVD dim\n",
    "    probe = TfidfVectorizer(max_features=max_features, min_df=mindf, ngram_range=(1,2))\n",
    "    Xp = probe.fit_transform(corpus_review)  # global probe is fine for dimension decisions\n",
    "    max_dim = max(2, min(256, Xp.shape[1]-1))\n",
    "    dim = max_dim if svd_dim is None else min(svd_dim, max_dim)\n",
    "    return Pipeline([\n",
    "        (\"vec\", TfidfVectorizer(max_features=max_features, min_df=mindf, ngram_range=(1,2))),\n",
    "        (\"svd\", TruncatedSVD(n_components=dim, random_state=SEED)),\n",
    "        (\"sc\",  StandardScaler(with_mean=False)),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000, n_jobs=None))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac818d5",
   "metadata": {},
   "source": [
    "## Cross-validated sanity check (small & fast)\n",
    "\n",
    "- Dynamic n_splits respects the minority class size to avoid errors on small subsets.\n",
    "\n",
    "- We report Accuracy, F1, ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4cdf0d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Count+LR (review)] Acc=0.896 F1=0.937 AUC=0.928\n",
      "[TFIDF+LR (review)] Acc=0.884 F1=0.932 AUC=0.944\n",
      "[TFIDF→SVD+LR (review)] Acc=0.891 F1=0.935 AUC=0.932\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TFIDF+LR (review)</td>\n",
       "      <td>0.884447</td>\n",
       "      <td>0.932459</td>\n",
       "      <td>0.943930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TFIDF→SVD+LR (review)</td>\n",
       "      <td>0.890856</td>\n",
       "      <td>0.934698</td>\n",
       "      <td>0.932440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Count+LR (review)</td>\n",
       "      <td>0.895535</td>\n",
       "      <td>0.937097</td>\n",
       "      <td>0.928375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name  accuracy        f1   roc_auc\n",
       "1      TFIDF+LR (review)  0.884447  0.932459  0.943930\n",
       "2  TFIDF→SVD+LR (review)  0.890856  0.934698  0.932440\n",
       "0      Count+LR (review)  0.895535  0.937097  0.928375"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell 7. Quick CV sanity check ===\n",
    "def eval_model(name: str, pipe: Pipeline, X: List[str], y: np.ndarray, seed: int = SEED) -> Dict[str, float]:\n",
    "    # Pick a safe number of folds\n",
    "    min_class = min((y==0).sum(), (y==1).sum())\n",
    "    n_splits = max(2, min(5, int(min_class)))  # at least 2, at most 5, not exceeding minority count\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    scores = {}\n",
    "    for metric in (\"accuracy\", \"f1\", \"roc_auc\"):\n",
    "        scores[metric] = cross_val_score(pipe, X, y, cv=cv, scoring=metric).mean()\n",
    "    print(f\"[{name}] Acc={scores['accuracy']:.3f} F1={scores['f1']:.3f} AUC={scores['roc_auc']:.3f}\")\n",
    "    return {\"name\": name, **scores}\n",
    "\n",
    "results = []\n",
    "results.append(eval_model(\"Count+LR (review)\",         make_count_lr(len(corpus_review)),    corpus_review, y))\n",
    "results.append(eval_model(\"TFIDF+LR (review)\",         make_tfidf_lr(len(corpus_review)),    corpus_review, y))\n",
    "results.append(eval_model(\"TFIDF→SVD+LR (review)\",     make_tfidf_svd_lr(len(corpus_review)), corpus_review, y))\n",
    "\n",
    "pd.DataFrame(results).sort_values(\"roc_auc\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ac6e5",
   "metadata": {},
   "source": [
    "## Train final models on all data & export bundle\n",
    "\n",
    "We train three pipelines on all review texts and export an equal-weight ensemble:\n",
    "\n",
    "- ensemble.pkl: contains models + predict/predict_proba logic\n",
    "\n",
    "- manifest.json: quick summary for your /metrics page\n",
    "\n",
    "\n",
    "\n",
    "The Flask app can simply joblib.load(\"model/ensemble.pkl\") and call:\n",
    "\n",
    "bundle.predict_proba([text]) or bundle.predict([text])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d5c85e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model bundle → /Users/mac/Desktop/dem-web/model/ensemble.pkl\n",
      "Saved manifest     → /Users/mac/Desktop/dem-web/model/manifest.json\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8. Train final models & export ===\n",
    "@dataclass\n",
    "class EnsembleBundle:\n",
    "    name: str\n",
    "    models: Dict[str, Any]          # {\"count_lr\": pipe, \"tfidf_lr\": pipe, \"svd_lr\": pipe}\n",
    "    weights: Dict[str, float]       # {\"count_lr\": 1/3, ...}\n",
    "    tokenizer_version: str = \"regex+light\"\n",
    "    notes: str = \"Milestone II export; all models trained on Review Text.\"\n",
    "\n",
    "    def _probas(self, texts: List[str]) -> Dict[str, np.ndarray]:\n",
    "        out = {}\n",
    "        for k, m in self.models.items():\n",
    "            p = m.predict_proba(texts)[:, 1]  # class 1 prob\n",
    "            out[k] = p\n",
    "        return out\n",
    "\n",
    "    def predict_proba(self, texts: List[str]) -> np.ndarray:\n",
    "        probs = self._probas(texts)\n",
    "        s = np.zeros(len(texts), dtype=float)\n",
    "        for k, p in probs.items():\n",
    "            w = self.weights.get(k, 0.0)\n",
    "            s += w * p\n",
    "        wsum = sum(self.weights.values()) or 1.0\n",
    "        return s / wsum\n",
    "\n",
    "    def predict(self, texts: List[str], threshold: float = 0.5) -> np.ndarray:\n",
    "        return (self.predict_proba(texts) >= threshold).astype(int)\n",
    "\n",
    "# Train three models on full corpus\n",
    "model_A = make_count_lr(len(corpus_review))\n",
    "model_B = make_tfidf_lr(len(corpus_review))\n",
    "model_C = make_tfidf_svd_lr(len(corpus_review))\n",
    "\n",
    "model_A.fit(corpus_review, y)\n",
    "model_B.fit(corpus_review, y)\n",
    "model_C.fit(corpus_review, y)\n",
    "\n",
    "weights = {\"count_lr\": 1.0, \"tfidf_lr\": 1.0, \"svd_lr\": 1.0}\n",
    "bundle = EnsembleBundle(\n",
    "    name=\"Count+TFIDF+LSA LR (equal-weight ensemble)\",\n",
    "    models={\"count_lr\": model_A, \"tfidf_lr\": model_B, \"svd_lr\": model_C},\n",
    "    weights=weights,\n",
    ")\n",
    "\n",
    "joblib.dump(bundle, BUNDLE_PKL)\n",
    "\n",
    "manifest = {\n",
    "    \"bundle_name\": bundle.name,\n",
    "    \"weights\": bundle.weights,\n",
    "    \"tokenizer\": bundle.tokenizer_version,\n",
    "    \"notes\": bundle.notes,\n",
    "    \"samples\": len(corpus_review),\n",
    "    \"positives\": int(y.sum()),\n",
    "    \"data_csv\": str(DATA_CSV)\n",
    "}\n",
    "with open(MANIFEST_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Saved model bundle → {BUNDLE_PKL}\")\n",
    "print(f\"Saved manifest     → {MANIFEST_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cec511",
   "metadata": {},
   "source": [
    "## What to wire in Flask (Milestone II)\n",
    "\n",
    "- Search: load data/site_items.json; normalize user query with the same normalize_for_search,\n",
    "\n",
    "rank items whose search_text contains any/most query tokens; show previews; link to detail pages.\n",
    "\n",
    "- New Review: take title + review_text, call bundle.predict_proba([text]) and display the\n",
    "\n",
    "suggested recommended label (allow override); on submit, append to your storage and show the new review page.\n",
    "\n",
    "- Metrics page: read model/manifest.json and show weights, bundle name, notes, sample counts.\n",
    "\n",
    "\n",
    "\n",
    "This notebook already created everything your app reads at runtime:\n",
    "\n",
    "- data/site_items.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25736753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
