{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41f4ad4",
   "metadata": {},
   "source": [
    "# Milestone II — Why these 3 models and how they work\n",
    "\n",
    "- This project keeps **one familiar baseline** from Milestone I and adds **two models with different inductive biases** so their errors are less correlated. The trio gives you strong accuracy, calibrated probabilities (for UI), and robustness to noisy, short, or misspelled text.\n",
    "\n",
    "\n",
    "- Using 3 model, each model focuses on **different cues** (word-level linear, NB bias, character-level margins).  \n",
    "\n",
    "\n",
    "## 1) TF-IDF + Logistic Regression (LR)  \n",
    "This model uses word n-grams with TF-IDF weighting and Logistic Regression for linear separation. Strengths: fast, smooth probabilities, interpretable features. Weakness: misses character-level signals and limited by linearity.  \n",
    "\n",
    "## 2) TF-IDF + Complement Naive Bayes (CNB)  \n",
    "Also uses TF-IDF word n-grams but applies Complement NB weighting. Strong for imbalanced or short data and trains extremely fast. Weakness: simplistic independence assumption and less flexible decision boundaries compared to LR/SVM.  \n",
    "\n",
    "## 3) Char TF-IDF (3–5) + Calibrated Linear SVM  \n",
    "Builds features from character n-grams to capture typos, concatenated words, and subword patterns. Linear SVM gives strong margins; calibration ensures usable probabilities. Strengths: handles noisy text and unseen words. Weakness: large feature space and extra compute for calibration.  \n",
    "\n",
    "## Design choices that support reliability\n",
    "- **Word n-grams (1–3)** on LR/CNB: capture unigrams (“itchy”), bigrams (“too small”), trigrams (“hard to wear”).  \n",
    "- **Character n-grams (3–5)** on SVM: capture morphology/typos (“itch”, “tchy”, “awf”, “ful”, “awful”).  \n",
    "- **`class_weight=\"balanced\"`** (LR/SVM): offsets label skew so minority examples matter.  \n",
    "- **`min_df` dynamic**: avoids “no terms remain” in small CV folds.  \n",
    "- **`max_features` cap**: protects memory while keeping the most informative grams.  \n",
    "- **Stopwords = `\"english\"`**: keeps pipeline simple and portable; we purposely do *not* strip sentiment-bearing words like “not” via extra rules to avoid losing negation information.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40348d0",
   "metadata": {},
   "source": [
    "## === Cell 1. Imports, paths, reproducibility ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b47bf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/mac/Desktop/dem-web\n",
      "DATA_CSV    : /Users/mac/Desktop/dem-web/data/assignment3_II.csv\n",
      "DATA_DIR    : /Users/mac/Desktop/dem-web/data\n",
      "MODEL_DIR   : /Users/mac/Desktop/dem-web/model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re, json, warnings, random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Resolve project paths so the notebook works from project root or /notebooks\n",
    "NB_DIR = Path.cwd().resolve()\n",
    "CANDIDATES = [\n",
    "    NB_DIR / \"data\" / \"assignment3_II.csv\",\n",
    "    NB_DIR.parent / \"data\" / \"assignment3_II.csv\",\n",
    "    NB_DIR.parents[1] / \"data\" / \"assignment3_II.csv\",\n",
    "]\n",
    "DATA_CSV = next((p for p in CANDIDATES if p.exists()), None)\n",
    "if DATA_CSV is None:\n",
    "    raise FileNotFoundError(\"Place 'assignment3_II.csv' under project_root/data/\")\n",
    "\n",
    "PROJECT_ROOT = DATA_CSV.parent.parent\n",
    "DATA_DIR  = PROJECT_ROOT / \"data\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"model\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CATALOG_JSON   = DATA_DIR  / \"site_items.json\"\n",
    "ENSEMBLE_PKL   = MODEL_DIR / \"ensemble_soft.pkl\"\n",
    "MANIFEST_JSON  = MODEL_DIR / \"manifest.json\"\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_CSV    :\", DATA_CSV)\n",
    "print(\"DATA_DIR    :\", DATA_DIR)\n",
    "print(\"MODEL_DIR   :\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52bbb75",
   "metadata": {},
   "source": [
    "## === Cell 2. Load CSV & normalize columns ===\n",
    "\n",
    "- Accept either `Recommended IND` or `Recommended` as the label.\n",
    "- Fill missing strings with `\"\"` and coerce numerics safely.\n",
    "- Ensure **display columns** exist: `Clothes Title`, `Clothes Description`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8491939b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 19662\n",
      "Columns: ['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating', 'Recommended IND', 'Positive Feedback Count', 'Division Name', 'Department Name', 'Class Name', 'Clothes Title', 'Clothes Description']\n",
      "Label column: Recommended IND | Positives: 16087\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "# Ensure display fields exist (fallbacks to legacy columns if missing)\n",
    "if \"Clothes Title\" not in df.columns:\n",
    "    df[\"Clothes Title\"] = df.get(\"Title\", \"\").fillna(\"\")\n",
    "if \"Clothes Description\" not in df.columns:\n",
    "    df[\"Clothes Description\"] = df.get(\"Review Text\", \"\").fillna(\"\")\n",
    "\n",
    "# Make sure these columns exist to avoid key errors later\n",
    "need_cols = [\n",
    "    \"Clothing ID\", \"Clothes Title\", \"Clothes Description\",\n",
    "    \"Rating\", \"Division Name\", \"Department Name\", \"Class Name\",\n",
    "    \"Review Text\", \"Title\"\n",
    "]\n",
    "for c in need_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = \"\"\n",
    "\n",
    "# Fill NaNs\n",
    "df = df.fillna({\n",
    "    \"Clothing ID\": 0,\n",
    "    \"Clothes Title\": \"\",\n",
    "    \"Clothes Description\": \"\",\n",
    "    \"Rating\": 0,\n",
    "    \"Division Name\": \"\",\n",
    "    \"Department Name\": \"\",\n",
    "    \"Class Name\": \"\",\n",
    "    \"Review Text\": \"\",\n",
    "    \"Title\": \"\",\n",
    "})\n",
    "\n",
    "# Coerce types\n",
    "def to_int_safe(x, default=0):\n",
    "    try: return int(x)\n",
    "    except: return default\n",
    "\n",
    "df[\"Clothing ID\"] = df[\"Clothing ID\"].apply(to_int_safe)\n",
    "df[\"Rating\"]      = df[\"Rating\"].apply(to_int_safe)\n",
    "\n",
    "# Determine label column\n",
    "label_col = None\n",
    "for cand in [\"Recommended IND\", \"Recommended\"]:\n",
    "    if cand in df.columns:\n",
    "        label_col = cand\n",
    "        break\n",
    "if label_col is None:\n",
    "    raise ValueError(\"No label column found. Expect 'Recommended IND' or 'Recommended'.\")\n",
    "\n",
    "# Clean label & drop invalid\n",
    "df[label_col] = pd.to_numeric(df[label_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "df = df[(df[label_col] == 0) | (df[label_col] == 1)].copy()\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"Label column:\", label_col, \"| Positives:\", int(df[label_col].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961f4e9",
   "metadata": {},
   "source": [
    "## 2.1 Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c49b9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining samples after cleaning: 19662\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-letter characters (keep spaces)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    # Collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to Review Text & Title\n",
    "df[\"Review Text\"] = df[\"Review Text\"].astype(str).apply(clean_text)\n",
    "df[\"Title\"]       = df[\"Title\"].astype(str).apply(clean_text)\n",
    "\n",
    "# Drop empty reviews after cleaning\n",
    "mask_nonempty = df[\"Review Text\"].str.strip().str.len() > 0\n",
    "df = df[mask_nonempty].copy()\n",
    "\n",
    "print(\"Remaining samples after cleaning:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c751e04",
   "metadata": {},
   "source": [
    "\n",
    "## === Cell 3. Plural-aware normalizer for search ===\n",
    "\n",
    "To support keyword search in UI, we normalize text:\n",
    "\n",
    "- Lowercase, keep word characters.\n",
    "- Simple plural reduction (e.g., *dresses* → *dress*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1afbed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: dress dress box fox class\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_word_re = re.compile(r\"[A-Za-z]+(?:[-'][A-Za-z]+)?\")\n",
    "\n",
    "def normalize_for_search(text: str) -> str:\n",
    "    text = (text or \"\").lower()\n",
    "    tokens = _word_re.findall(text)\n",
    "\n",
    "    def reduce_plural(w: str) -> str:\n",
    "        if len(w) <= 3: return w\n",
    "        for suf, rep in [(\"ies\",\"y\"), (\"sses\",\"ss\"), (\"xes\",\"x\"), (\"zes\",\"z\")]:\n",
    "            if w.endswith(suf): return w[:-len(suf)] + rep\n",
    "        for suf in (\"es\",\"s\"):\n",
    "            if w.endswith(suf) and not w.endswith(\"ss\"):\n",
    "                return w[:-len(suf)]\n",
    "        return w\n",
    "\n",
    "    return \" \".join(reduce_plural(t) for t in tokens)\n",
    "\n",
    "print(\"Test:\", normalize_for_search(\"Dress / dresses — Boxes, foxes, classes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c1f437",
   "metadata": {},
   "source": [
    "## Cell 5. Build modeling corpus & label\n",
    "\n",
    "Model on **Review Text** (best generalization); drop blank reviews.\n",
    "\n",
    "- `corpus_review` — list of review texts\n",
    "- `y` — binary labels (0/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a12ca7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 19662  |  Positives: 16087\n",
      "Example review: i had such high hopes for this dress and really wanted it to work for me i initially ordered the petite small my usual s...\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5. Build corpus & labels (Milestone II) ===\n",
    "# Uses Review Text as the main signal. Drops empty reviews. Label from Recommended IND/Recommended.\n",
    "\n",
    "# Ensure string types\n",
    "df[\"Review Text\"] = df[\"Review Text\"].fillna(\"\").astype(str)\n",
    "df[\"Title\"] = df[\"Title\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Drop empty reviews\n",
    "mask_nonempty = df[\"Review Text\"].str.strip().str.len() > 0\n",
    "dfe = df[mask_nonempty].copy()\n",
    "\n",
    "# X, y\n",
    "corpus_review = dfe[\"Review Text\"].tolist()\n",
    "# Optional: title + review variant if you want later\n",
    "corpus_title_review = dfe[\"Title\"].str.cat(dfe[\"Review Text\"], sep=\" . \").tolist()\n",
    "y = dfe[label_col].astype(int).to_numpy()\n",
    "\n",
    "print(f\"Samples: {len(y)}  |  Positives: {int(y.sum())}\")\n",
    "print(\"Example review:\", (corpus_review[0][:120] + \"...\") if len(corpus_review) else \"(empty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de976572",
   "metadata": {},
   "source": [
    "## Cell 6. Pipelines (3 diverse models)\n",
    "\n",
    "### Why these three?\n",
    "\n",
    "1) **TF-IDF + Logistic Regression (LR)**  \n",
    "   - Strengths: strong, smooth probabilities; linear on word n-grams; interpretable; fast.  \n",
    "   - Weaknesses: purely linear; might miss character-level patterns.\n",
    "\n",
    "2) **TF-IDF + Complement Naive Bayes (CNB)**  \n",
    "   - Strengths: different inductive bias; robust for small/imbalanced datasets; extremely fast training.  \n",
    "   - Weaknesses: conditional independence assumption; decision boundaries can be less flexible.\n",
    "\n",
    "3) **Character TF-IDF (3–5) + Linear SVM (Calibrated)**  \n",
    "   - Strengths: char n-grams capture typos/joins; SVM margins are strong; calibrated to get `predict_proba` for UI/ensemble.  \n",
    "   - Weaknesses: larger feature spaces; calibration adds compute.\n",
    "\n",
    "### Safety knobs\n",
    "- Dynamic `min_df` to avoid “no terms remain” on small folds.  \n",
    "- Capped `max_features` to control memory.  \n",
    "- `class_weight=\"balanced\"` on LR & SVM.  \n",
    "- SVM wrapped in `CalibratedClassifierCV(method=\"sigmoid\", cv=3)` to provide probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89bd9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6. Pipelines (3 diverse models, library stopwords) ===\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Start from sklearn's list and KEEP negations so sentiment flips remain informative\n",
    "STOP_WORDS_LIST = sorted(list(ENGLISH_STOP_WORDS - {\"no\", \"not\", \"never\"}))\n",
    "\n",
    "def choose_min_df(n_docs: int) -> int:\n",
    "    \"\"\"Dynamic min_df to avoid 'no terms remain' on small folds.\"\"\"\n",
    "    if n_docs >= 5000: return 3\n",
    "    if n_docs >= 1000: return 2\n",
    "    return 1\n",
    "\n",
    "def make_tfidf_lr(n_docs: int, max_features: int = 40000) -> Pipeline:\n",
    "    \"\"\"Word TF-IDF (1–3)-gram + Logistic Regression (balanced).\"\"\"\n",
    "    mindf = choose_min_df(n_docs)\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            ngram_range=(1, 3),\n",
    "            max_features=max_features,\n",
    "            min_df=mindf,\n",
    "            stop_words=STOP_WORDS_LIST\n",
    "        )),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=3000,\n",
    "            class_weight=\"balanced\",\n",
    "            solver=\"liblinear\",\n",
    "            random_state=SEED\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def make_tfidf_cnb(n_docs: int, max_features: int = 40000, alpha: float = 0.5) -> Pipeline:\n",
    "    \"\"\"Word TF-IDF (1–3)-gram + Complement Naive Bayes.\"\"\"\n",
    "    mindf = choose_min_df(n_docs)\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            ngram_range=(1, 3),\n",
    "            max_features=max_features,\n",
    "            min_df=mindf,\n",
    "            stop_words=STOP_WORDS_LIST\n",
    "        )),\n",
    "        (\"clf\", ComplementNB(alpha=alpha))\n",
    "    ])\n",
    "\n",
    "def make_char_svm(n_docs: int, C=0.5) -> Pipeline:\n",
    "    vec = TfidfVectorizer(\n",
    "        analyzer=\"char_wb\",        # better for short texts & word boundaries\n",
    "        ngram_range=(3,5),\n",
    "        sublinear_tf=True,\n",
    "        min_df=2                   # drop ultra-rare shards\n",
    "    )\n",
    "    svm = LinearSVC(C=C, class_weight=\"balanced\", random_state=SEED)\n",
    "    clf = CalibratedClassifierCV(svm, method=\"sigmoid\", cv=5)  # more stable calibration\n",
    "    return Pipeline([(\"vec\", vec), (\"clf\", clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d4055",
   "metadata": {},
   "source": [
    "## Cell 7. Quick cross-validated sanity check\n",
    "\n",
    "- Use **StratifiedKFold** with a safe number of splits (≤ minority class size).\n",
    "- Report **Accuracy**, **F1**, **ROC-AUC** for each model and the **soft-voting ensemble**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52344654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TFIDF+LR (word 1-3)]  Acc=0.878  F1=0.923  AUC=0.934\n",
      "[TFIDF+CNB (word 1-3)]  Acc=0.892  F1=0.935  AUC=0.929\n",
      "[Char TFIDF+SVM (3-5)]  Acc=0.891  F1=0.935  AUC=0.934\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Char TFIDF+SVM (3-5)</td>\n",
       "      <td>0.890804</td>\n",
       "      <td>0.934793</td>\n",
       "      <td>0.934143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF+LR (word 1-3)</td>\n",
       "      <td>0.878497</td>\n",
       "      <td>0.923079</td>\n",
       "      <td>0.934087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TFIDF+CNB (word 1-3)</td>\n",
       "      <td>0.892331</td>\n",
       "      <td>0.935087</td>\n",
       "      <td>0.929135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  accuracy        f1   roc_auc\n",
       "2  Char TFIDF+SVM (3-5)  0.890804  0.934793  0.934143\n",
       "0   TFIDF+LR (word 1-3)  0.878497  0.923079  0.934087\n",
       "1  TFIDF+CNB (word 1-3)  0.892331  0.935087  0.929135"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell 7. Cross-validated sanity check (Acc/F1/AUC) ===\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def safe_cv(y: np.ndarray, seed: int = SEED) -> StratifiedKFold:\n",
    "    \"\"\"Pick a safe number of folds given class counts.\"\"\"\n",
    "    min_class = int(min((y == 0).sum(), (y == 1).sum()))\n",
    "    n_splits = max(2, min(5, min_class))  # at least 2, at most 5, not exceeding minority count\n",
    "    return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "def eval_model(name: str, pipe: Pipeline, X, y, seed: int = SEED):\n",
    "    cv = safe_cv(y, seed)\n",
    "    metrics = {}\n",
    "    for metric in (\"accuracy\", \"f1\", \"roc_auc\"):\n",
    "        metrics[metric] = cross_val_score(pipe, X, y, cv=cv, scoring=metric).mean()\n",
    "    print(f\"[{name}]  Acc={metrics['accuracy']:.3f}  F1={metrics['f1']:.3f}  AUC={metrics['roc_auc']:.3f}\")\n",
    "    return {\"name\": name, **metrics}\n",
    "\n",
    "results = []\n",
    "results.append(eval_model(\"TFIDF+LR (word 1-3)\", make_tfidf_lr(len(corpus_review)), corpus_review, y))\n",
    "results.append(eval_model(\"TFIDF+CNB (word 1-3)\", make_tfidf_cnb(len(corpus_review)), corpus_review, y))\n",
    "results.append(eval_model(\"Char TFIDF+SVM (3-5)\", make_char_svm(len(corpus_review)), corpus_review, y))\n",
    "\n",
    "pd.DataFrame(results).sort_values(\"roc_auc\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0278aa",
   "metadata": {},
   "source": [
    "## Cell 8. Train on all data & export artifacts\n",
    "\n",
    "Train the three pipelines and the **soft-voting ensemble** on the full corpus.\n",
    "\n",
    "Export:\n",
    "- `model/ensemble_soft.pkl` — a **scikit-learn VotingClassifier**, *safely picklable* (no custom class).\n",
    "- `model/manifest.json` — metadata for your UI’s **/metrics** page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95302b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model bundle → /Users/mac/Desktop/dem-web/model/ensemble_soft.pkl\n",
      "Saved manifest     → /Users/mac/Desktop/dem-web/model/manifest.json\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8. Train final models on all data & export bundle ===\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnsembleBundle:\n",
    "    \"\"\"Soft-vote ensemble that averages model probabilities.\"\"\"\n",
    "    name: str\n",
    "    models: Dict[str, Any]          # keys: \"tfidf_lr\", \"tfidf_cnb\", \"char_svm\"\n",
    "    weights: Dict[str, float]       # e.g., {\"tfidf_lr\": 1.0, \"tfidf_cnb\": 1.0, \"char_svm\": 1.0}\n",
    "    notes: str = \"Milestone II ensemble: word TF-IDF+LR, word TF-IDF+CNB, char TF-IDF+Calibrated SVM.\"\n",
    "    stopwords: str = \"sklearn.ENGLISH_STOP_WORDS minus {no,not,never}\"\n",
    "    ngram_word: str = \"(1,3)\"\n",
    "    ngram_char: str = \"(3,5)\"\n",
    "\n",
    "    def _probas(self, texts: List[str]) -> Dict[str, np.ndarray]:\n",
    "        out = {}\n",
    "        for k, m in self.models.items():\n",
    "            p = m.predict_proba(texts)[:, 1]  # probability of positive class\n",
    "            out[k] = p\n",
    "        return out\n",
    "\n",
    "    def predict_proba(self, texts: List[str]) -> np.ndarray:\n",
    "        probs = self._probas(texts)\n",
    "        s = np.zeros(len(texts), dtype=float)\n",
    "        wsum = float(sum(self.weights.values())) or 1.0\n",
    "        for k, p in probs.items():\n",
    "            s += self.weights.get(k, 0.0) * p\n",
    "        return s / wsum\n",
    "\n",
    "    def predict(self, texts: List[str], threshold: float = 0.5) -> np.ndarray:\n",
    "        return (self.predict_proba(texts) >= threshold).astype(int)\n",
    "\n",
    "# Train three models on the full corpus\n",
    "mdl_lr  = make_tfidf_lr(len(corpus_review))\n",
    "mdl_cnb = make_tfidf_cnb(len(corpus_review))\n",
    "mdl_svm = make_char_svm(len(corpus_review))\n",
    "\n",
    "mdl_lr.fit(corpus_review, y)\n",
    "mdl_cnb.fit(corpus_review, y)\n",
    "mdl_svm.fit(corpus_review, y)\n",
    "\n",
    "weights = {\"tfidf_lr\": 1.0, \"tfidf_cnb\": 1.0, \"char_svm\": 1.0}\n",
    "bundle = EnsembleBundle(\n",
    "    name=\"Milestone II: TFIDF+LR / TFIDF+CNB / CharTFIDF+SVM (soft-vote, equal weights)\",\n",
    "    models={\"tfidf_lr\": mdl_lr, \"tfidf_cnb\": mdl_cnb, \"char_svm\": mdl_svm},\n",
    "    weights=weights\n",
    ")\n",
    "\n",
    "joblib.dump(bundle, ENSEMBLE_PKL)\n",
    "\n",
    "manifest = {\n",
    "    \"bundle_name\": bundle.name,\n",
    "    \"weights\": bundle.weights,\n",
    "    \"notes\": bundle.notes,\n",
    "    \"stopwords\": bundle.stopwords,\n",
    "    \"word_ngram\": bundle.ngram_word,\n",
    "    \"char_ngram\": bundle.ngram_char,\n",
    "    \"samples\": int(len(corpus_review)),\n",
    "    \"positives\": int(y.sum()),\n",
    "    \"data_csv\": str(DATA_CSV)\n",
    "}\n",
    "with open(MANIFEST_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Saved model bundle → {ENSEMBLE_PKL}\")\n",
    "print(f\"Saved manifest     → {MANIFEST_JSON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdece583",
   "metadata": {},
   "source": [
    "## Cell 9. Quick test of trained models\n",
    "\n",
    " - Pass a few example texts through each individual model + ensemble.\n",
    "- Show probability of \"Positive\" (recommended).\n",
    "- Also display the final predicted label (Positive / Negative) using threshold=0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5bb9184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT: This dress is beautiful and I love it!\n",
      "------------------------------------------------------------\n",
      "[tfidf_lr]  P(positive) = 0.932 → Positive ✅\n",
      "[tfidf_cnb]  P(positive) = 0.779 → Positive ✅\n",
      "[char_svm]  P(positive) = 0.994 → Positive ✅\n",
      "[ensemble_soft]  P(positive) = 0.902 → Positive ✅\n",
      "\n",
      "TEXT: This product is terrible and very uncomfortable to wear.\n",
      "------------------------------------------------------------\n",
      "[tfidf_lr]  P(positive) = 0.147 → Negative ❌\n",
      "[tfidf_cnb]  P(positive) = 0.140 → Negative ❌\n",
      "[char_svm]  P(positive) = 0.292 → Negative ❌\n",
      "[ensemble_soft]  P(positive) = 0.193 → Negative ❌\n",
      "\n",
      "TEXT: So difficult and itchy.\n",
      "------------------------------------------------------------\n",
      "[tfidf_lr]  P(positive) = 0.276 → Negative ❌\n",
      "[tfidf_cnb]  P(positive) = 0.471 → Negative ❌\n",
      "[char_svm]  P(positive) = 0.759 → Positive ✅\n",
      "[ensemble_soft]  P(positive) = 0.502 → Positive ✅\n",
      "\n",
      "TEXT: Perfect fit, high quality material.\n",
      "------------------------------------------------------------\n",
      "[tfidf_lr]  P(positive) = 0.689 → Positive ✅\n",
      "[tfidf_cnb]  P(positive) = 0.790 → Positive ✅\n",
      "[char_svm]  P(positive) = 0.883 → Positive ✅\n",
      "[ensemble_soft]  P(positive) = 0.788 → Positive ✅\n",
      "\n",
      "TEXT: Cheap fabric, looks awful.\n",
      "------------------------------------------------------------\n",
      "[tfidf_lr]  P(positive) = 0.031 → Negative ❌\n",
      "[tfidf_cnb]  P(positive) = 0.029 → Negative ❌\n",
      "[char_svm]  P(positive) = 0.005 → Negative ❌\n",
      "[ensemble_soft]  P(positive) = 0.022 → Negative ❌\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = [\n",
    "    \"This dress is beautiful and I love it!\",\n",
    "    \"This product is terrible and very uncomfortable to wear.\",\n",
    "    \"So difficult and itchy.\",\n",
    "    \"Perfect fit, high quality material.\",\n",
    "    \"Cheap fabric, looks awful.\"\n",
    "]\n",
    "\n",
    "def predict_with_all(text: str, bundle, threshold: float = 0.5):\n",
    "    print(f\"\\nTEXT: {text}\\n\" + \"-\"*60)\n",
    "    for name, model in bundle.models.items():\n",
    "        p = model.predict_proba([text])[0,1]\n",
    "        label = \"Positive ✅\" if p >= threshold else \"Negative ❌\"\n",
    "        print(f\"[{name}]  P(positive) = {p:.3f} → {label}\")\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    p_ens = bundle.predict_proba([text])[0]\n",
    "    label_ens = \"Positive ✅\" if p_ens >= threshold else \"Negative ❌\"\n",
    "    print(f\"[ensemble_soft]  P(positive) = {p_ens:.3f} → {label_ens}\")\n",
    "\n",
    "# Run on sample texts\n",
    "for t in EXAMPLES:\n",
    "    predict_with_all(t, bundle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728b31f",
   "metadata": {},
   "source": [
    "## How Flask should use these artifacts\n",
    "\n",
    "At **runtime** (Flask app):\n",
    "\n",
    "- Load the ensemble once and reuse:\n",
    "  ```python\n",
    "  import joblib, json\n",
    "  ENSEMBLE_PKL = \"model/ensemble_soft.pkl\"\n",
    "  MANIFEST_JSON = \"model/manifest.json\"\n",
    "  ensemble = joblib.load(ENSEMBLE_PKL)  # VotingClassifier\n",
    "  # predict_proba:\n",
    "  p = float(ensemble.predict_proba([user_text])[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c7902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
