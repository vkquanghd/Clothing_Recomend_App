{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41f4ad4",
   "metadata": {},
   "source": [
    "\n",
    " # Milestone II — Why these 3 models and how they work\n",
    "\n",
    " We keep one familiar baseline and add two models with different inductive biases so their errors are less correlated.\n",
    " The trio gives strong accuracy, calibrated probabilities (needed for UI), and robustness to noisy/misspelled text.\n",
    "\n",
    " - 3 models, 3 views of the text: word-level linear, NB bias, character-level margins.\n",
    "\n",
    " ## 1) TF-IDF + Logistic Regression (LR)\n",
    " Word n-grams + LR. Fast, smooth probabilities, interpretable features. Misses char clues; linear boundary.\n",
    "\n",
    " ## 2) TF-IDF + Complement Naive Bayes (CNB)\n",
    " Word n-grams + CNB. Strong on short/imbalanced data, trains very fast. Independence assumption limits boundary.\n",
    "\n",
    " ## 3) Char TF-IDF (3–5) + Calibrated Linear SVM\n",
    " Character n-grams capture typos/subword patterns. SVM margins are strong; calibration yields probabilities.\n",
    "\n",
    " **Safety knobs**: dynamic `min_df`, capped `max_features`, `class_weight=\"balanced\"` (LR/SVM), SVM probability calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40348d0",
   "metadata": {},
   "source": [
    "## === Cell 1. Imports, paths, reproducibility ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b47bf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/mac/Desktop/dem-web\n",
      "DATA_CSV    : /Users/mac/Desktop/dem-web/data/assignment3_II.csv\n",
      "MODEL_DIR   : /Users/mac/Desktop/dem-web/model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json, warnings, random, re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Resolve dataset no matter where the notebook runs\n",
    "NB_DIR = Path.cwd().resolve()\n",
    "CANDIDATES = [\n",
    "    NB_DIR / \"data\" / \"assignment3_II.csv\",\n",
    "    NB_DIR.parent / \"data\" / \"assignment3_II.csv\",\n",
    "    NB_DIR.parents[1] / \"data\" / \"assignment3_II.csv\",\n",
    "]\n",
    "DATA_CSV = next((p for p in CANDIDATES if p.exists()), None)\n",
    "if DATA_CSV is None:\n",
    "    raise FileNotFoundError(\"Place 'assignment3_II.csv' under project_root/data/\")\n",
    "\n",
    "PROJECT_ROOT = DATA_CSV.parent.parent\n",
    "DATA_DIR  = PROJECT_ROOT / \"data\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"model\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ENSEMBLE_PKL   = MODEL_DIR / \"ensemble_soft.pkl\"\n",
    "MANIFEST_JSON  = MODEL_DIR / \"manifest.json\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_CSV    :\", DATA_CSV)\n",
    "print(\"MODEL_DIR   :\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52bbb75",
   "metadata": {},
   "source": [
    "## === Cell 2. Load CSV & normalize columns ===\n",
    " - Accept either `Recommended IND` or `Recommended` as label.\n",
    " - Minimal cleaning; keep \"not/never/no\" in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8491939b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 19662\n",
      "Label: Recommended IND | Positives: 16087\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "\n",
    "# Display fields (fallbacks)\n",
    "df[\"Clothes Title\"] = df.get(\"Clothes Title\", df.get(\"Title\", \"\")).fillna(\"\")\n",
    "df[\"Clothes Description\"] = df.get(\"Clothes Description\", df.get(\"Review Text\", \"\")).fillna(\"\")\n",
    "\n",
    "# Ensure required columns exist\n",
    "for c in [\n",
    "    \"Clothing ID\",\"Clothes Title\",\"Clothes Description\",\"Rating\",\n",
    "    \"Division Name\",\"Department Name\",\"Class Name\",\"Review Text\",\"Title\"\n",
    "]:\n",
    "    if c not in df.columns:\n",
    "        df[c] = \"\"\n",
    "\n",
    "# Types\n",
    "def to_int_safe(x, default=0):\n",
    "    try: return int(x)\n",
    "    except: return default\n",
    "\n",
    "df[\"Clothing ID\"] = df[\"Clothing ID\"].apply(to_int_safe)\n",
    "df[\"Rating\"]      = pd.to_numeric(df[\"Rating\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df[\"Review Text\"] = df[\"Review Text\"].fillna(\"\").astype(str)\n",
    "df[\"Title\"]       = df[\"Title\"].fillna(\"\").astype(str)\n",
    "\n",
    "# Label column\n",
    "label_col = \"Recommended IND\" if \"Recommended IND\" in df.columns else (\n",
    "            \"Recommended\" if \"Recommended\" in df.columns else None)\n",
    "if label_col is None:\n",
    "    raise ValueError(\"No label found. Expect 'Recommended IND' or 'Recommended'.\")\n",
    "\n",
    "df[label_col] = pd.to_numeric(df[label_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "df = df[(df[label_col].isin([0,1]))].copy()\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Label:\", label_col, \"| Positives:\", int(df[label_col].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9961f4e9",
   "metadata": {},
   "source": [
    "## === Cell 3. Light text cleaning for modeling ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c49b9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining samples: 19662\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s']\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "df[\"Review Text\"] = df[\"Review Text\"].apply(clean_text)\n",
    "df[\"Title\"]       = df[\"Title\"].apply(clean_text)\n",
    "\n",
    "mask_nonempty = df[\"Review Text\"].str.len() > 0\n",
    "df = df[mask_nonempty].copy()\n",
    "\n",
    "print(\"Remaining samples:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c751e04",
   "metadata": {},
   "source": [
    "## === Cell 4. Build corpus & labels ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1afbed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 19662 | Positives: 16087\n",
      "Example: i had such high hopes for this dress and really wanted it to work for me i initially ordered the petite small my usual s...\n"
     ]
    }
   ],
   "source": [
    "X = df[\"Review Text\"].tolist()\n",
    "y = df[label_col].astype(int).to_numpy()\n",
    "\n",
    "print(f\"Samples: {len(y)} | Positives: {int(y.sum())}\")\n",
    "print(\"Example:\", (X[0][:120] + \"...\") if X else \"(empty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c1f437",
   "metadata": {},
   "source": [
    "## === Cell 5. Define the 3 pipelines ===\n",
    " - Word TF-IDF (1–3) + LR (balanced).\n",
    " - Word TF-IDF (1–3) + Complement NB.\n",
    " - Char TF-IDF (3–5) + LinearSVC (calibrated to get `predict_proba`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a12ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42\n",
    "\n",
    "# Keep negations/intensity words; remove most of the stopwords\n",
    "STOP_WORDS_KEEP_NEG = sorted(list(ENGLISH_STOP_WORDS - {\"no\", \"not\", \"never\", \"too\", \"very\"}))\n",
    "\n",
    "def choose_min_df(n_docs: int) -> int:\n",
    "    \"\"\"Choose a small min_df so rare bigrams like 'too tight' stay in the vocab.\"\"\"\n",
    "    return 1 if n_docs < 5000 else 2\n",
    "\n",
    "def make_tfidf_lr(n_docs: int, max_features: int = 40_000) -> Pipeline:\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            ngram_range=(1, 3),\n",
    "            max_features=max_features,\n",
    "            min_df=choose_min_df(n_docs),\n",
    "            stop_words=STOP_WORDS_KEEP_NEG,\n",
    "            sublinear_tf=True\n",
    "        )),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            class_weight=\"balanced\",\n",
    "            solver=\"liblinear\",\n",
    "            random_state=SEED\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def make_tfidf_cnb(n_docs: int, max_features: int = 40_000, alpha: float = 0.5) -> Pipeline:\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            ngram_range=(1, 3),\n",
    "            max_features=max_features,\n",
    "            min_df=choose_min_df(n_docs),\n",
    "            stop_words=STOP_WORDS_KEEP_NEG,\n",
    "            sublinear_tf=True\n",
    "        )),\n",
    "        (\"clf\", ComplementNB(alpha=alpha))\n",
    "    ])\n",
    "\n",
    "def make_char_svm(n_docs: int, C: float = 0.5) -> Pipeline:\n",
    "    # char_wb is strong on misspellings/short phrases\n",
    "    vec = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), sublinear_tf=True, min_df=2)\n",
    "    svm = LinearSVC(C=C, class_weight=\"balanced\", random_state=SEED)\n",
    "    cal = CalibratedClassifierCV(svm, method=\"sigmoid\", cv=3)  # enables predict_proba\n",
    "    return Pipeline([(\"vec\", vec), (\"clf\", cal)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de976572",
   "metadata": {},
   "source": [
    "## == Cell 6. Build X (corpus) and y (labels) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89bd9ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell 6] Samples kept: 19662 | Positives: 16087 | Negatives: 3575\n",
      "Example: i had such high hopes for this dress and really wanted it to work for me i initially ordered the petite small my usual s...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def _clean_text(s: str) -> str:\n",
    "    s = (s or \"\").lower()\n",
    "    # keep basic punctuation that can form bigrams like \"too tight\"\n",
    "    s = re.sub(r\"[^a-z0-9\\s.,!'?-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "dfe = df.copy()\n",
    "dfe[\"Review Text\"] = dfe[\"Review Text\"].fillna(\"\").astype(str).map(_clean_text)\n",
    "\n",
    "y_all = pd.to_numeric(dfe[label_col], errors=\"coerce\").fillna(0).astype(int)\n",
    "mask = (dfe[\"Review Text\"].str.len() > 0) & (y_all.isin([0, 1]))\n",
    "dfe = dfe.loc[mask].reset_index(drop=True)\n",
    "\n",
    "# Final arrays\n",
    "X = dfe[\"Review Text\"].tolist()\n",
    "y = dfe[label_col].astype(int).to_numpy()\n",
    "\n",
    "print(f\"[Cell 6] Samples kept: {len(y)} | Positives: {int(y.sum())} | Negatives: {len(y) - int(y.sum())}\")\n",
    "print(\"Example:\", (X[0][:120] + \"...\") if len(X) else \"(empty)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d4055",
   "metadata": {},
   "source": [
    "## === Cell 7. Quick CV sanity check (Acc/F1/AUC) ===\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52344654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TFIDF+LR (1–3)]  Acc=0.881  F1=0.924  AUC=0.936\n",
      "[TFIDF+CNB (1–3)]  Acc=0.893  F1=0.936  AUC=0.930\n",
      "[Char TFIDF+SVM (3–5)]  Acc=0.892  F1=0.936  AUC=0.935\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TFIDF+LR (1–3)</td>\n",
       "      <td>0.880582</td>\n",
       "      <td>0.924484</td>\n",
       "      <td>0.935627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Char TFIDF+SVM (3–5)</td>\n",
       "      <td>0.892279</td>\n",
       "      <td>0.935727</td>\n",
       "      <td>0.934982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TFIDF+CNB (1–3)</td>\n",
       "      <td>0.892839</td>\n",
       "      <td>0.935686</td>\n",
       "      <td>0.930439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  accuracy        f1   roc_auc\n",
       "0        TFIDF+LR (1–3)  0.880582  0.924484  0.935627\n",
       "2  Char TFIDF+SVM (3–5)  0.892279  0.935727  0.934982\n",
       "1       TFIDF+CNB (1–3)  0.892839  0.935686  0.930439"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def _safe_cv(y_vec: np.ndarray, seed: int = SEED) -> StratifiedKFold:\n",
    "    min_class = int(min((y_vec == 0).sum(), (y_vec == 1).sum()))\n",
    "    n_splits = max(2, min(5, min_class))\n",
    "    return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "def _eval_model(name: str, pipe: Pipeline, X_list, y_vec):\n",
    "    cv = _safe_cv(y_vec)\n",
    "    metrics = {}\n",
    "    for metric in (\"accuracy\", \"f1\", \"roc_auc\"):\n",
    "        metrics[metric] = cross_val_score(pipe, X_list, y_vec, cv=cv, scoring=metric).mean()\n",
    "    print(f\"[{name}]  Acc={metrics['accuracy']:.3f}  F1={metrics['f1']:.3f}  AUC={metrics['roc_auc']:.3f}\")\n",
    "    return {\"name\": name, **metrics}\n",
    "\n",
    "results = []\n",
    "results.append(_eval_model(\"TFIDF+LR (1–3)\",  make_tfidf_lr(len(X)),  X, y))\n",
    "results.append(_eval_model(\"TFIDF+CNB (1–3)\", make_tfidf_cnb(len(X)), X, y))\n",
    "results.append(_eval_model(\"Char TFIDF+SVM (3–5)\", make_char_svm(len(X)), X, y))\n",
    "\n",
    "pd.DataFrame(results).sort_values(\"roc_auc\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0278aa",
   "metadata": {},
   "source": [
    "## === Cell 8. Train on all data & export artifacts ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95302b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → /Users/mac/Desktop/dem-web/model/ensemble_soft.pkl\n",
      "Saved → /Users/mac/Desktop/dem-web/model/manifest.json\n",
      "\n",
      "Sanity check (raw -> adjusted -> label):\n",
      "- Lovely ugly top, but too tight\n",
      "  raw=0.614  adj=0.534  thr=0.60  → Negative\n",
      "\n",
      "- This dress is beautiful and I love it!\n",
      "  raw=0.915  adj=0.915  thr=0.60  → Positive\n",
      "\n",
      "- Cheap fabric and very uncomfortable, runs small\n",
      "  raw=0.074  adj=0.000  thr=0.60  → Negative\n",
      "\n",
      "- Perfect fit, high quality material.\n",
      "  raw=0.798  adj=0.798  thr=0.60  → Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train base models\n",
    "mdl_lr  = make_tfidf_lr(len(X)).fit(X, y)\n",
    "mdl_cnb = make_tfidf_cnb(len(X)).fit(X, y)\n",
    "mdl_svm = make_char_svm(len(X)).fit(X, y)\n",
    "\n",
    "# Slightly higher weight on char model (often helps short/misspelled reviews)\n",
    "VOTE_WEIGHTS = (0.9, 0.9, 1.2)\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"tfidf_lr\",  mdl_lr),\n",
    "        (\"tfidf_cnb\", mdl_cnb),\n",
    "        (\"char_svm\",  mdl_svm),\n",
    "    ],\n",
    "    voting=\"soft\",\n",
    "    weights=list(VOTE_WEIGHTS),\n",
    "    n_jobs=None\n",
    ").fit(X, y)\n",
    "\n",
    "# --- Runtime post-probability heuristic (documented & used by API) ---\n",
    "# Keep this set in sync with model_info.py or let the API read from manifest.\n",
    "NEG_PHRASES = {\n",
    "    \"too tight\", \"too small\", \"itchy\", \"scratchy\", \"see through\",\n",
    "    \"cheap fabric\", \"poor fit\", \"returned\", \"uncomfortable\",\n",
    "    \"runs small\", \"runs large\"\n",
    "}\n",
    "HEURISTIC_DELTA = 0.08        # <-- how much to nudge down per hit (API uses this)\n",
    "THRESHOLD = 0.60              # <-- decision threshold used by API\n",
    "\n",
    "# Save the pure sklearn ensemble (joblib)\n",
    "joblib.dump(ensemble, ENSEMBLE_PKL)\n",
    "\n",
    "# Save manifest with parameters the Flask app reads\n",
    "manifest = {\n",
    "    \"bundle_name\": \"VotingClassifier (LR + CNB + CharSVM)\",\n",
    "    \"weights\": {\"tfidf_lr\": VOTE_WEIGHTS[0], \"tfidf_cnb\": VOTE_WEIGHTS[1], \"char_svm\": VOTE_WEIGHTS[2]},\n",
    "    \"threshold\": THRESHOLD,\n",
    "    \"post_prob_heuristic\": True,\n",
    "    \"heuristic_delta\": HEURISTIC_DELTA,\n",
    "    \"neg_phrases\": sorted(list(NEG_PHRASES)),\n",
    "    \"word_ngram\": \"(1,3)\",\n",
    "    \"char_ngram\": \"(3,5)\",\n",
    "    \"samples\": int(len(X)),\n",
    "    \"positives\": int(y.sum()),\n",
    "    \"notes\": \"Keep negation/intensity tokens; slight char-model upweight; optional heuristic nudge (too tight, runs small, ...).\"\n",
    "}\n",
    "with open(MANIFEST_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Saved → {ENSEMBLE_PKL}\")\n",
    "print(f\"Saved → {MANIFEST_JSON}\")\n",
    "\n",
    "# --- Optional: quick sanity check mirroring the API (raw vs adjusted vs label) ---\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def _post_prob_adjust(text: str, p: float, phrases=NEG_PHRASES, delta=HEURISTIC_DELTA, enabled=True) -> float:\n",
    "    \"\"\"Reduce prob if any negative phrase is present; clamp to [0,1].\"\"\"\n",
    "    if not enabled:\n",
    "        return p\n",
    "    s = (text or \"\").lower()\n",
    "    hits = sum(1 for ph in phrases if ph in s)\n",
    "    return max(0.0, min(1.0, p - hits * delta))\n",
    "\n",
    "def _decide_label(prob: float, thr: float = THRESHOLD) -> str:\n",
    "    return \"Positive\" if prob >= thr else \"Negative\"\n",
    "\n",
    "def predict_like_api(model, text: str) -> tuple[str, float, float]:\n",
    "    \"\"\"Return (label, prob_adjusted, prob_raw) using the same rules as the API.\"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        raw = float(model.predict_proba([text])[:, 1][0])\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        s = float(model.decision_function([text])[0])\n",
    "        raw = float(1.0 / (1.0 + np.exp(-s)))\n",
    "    else:\n",
    "        yhat = int(model.predict([text])[0])\n",
    "        raw = 0.75 if yhat == 1 else 0.25\n",
    "    adj = _post_prob_adjust(text, raw, phrases=NEG_PHRASES, delta=HEURISTIC_DELTA, enabled=True)\n",
    "    lab = _decide_label(adj, thr=THRESHOLD)\n",
    "    return lab, adj, raw\n",
    "\n",
    "# Reload the exact saved model to simulate Flask runtime\n",
    "_model_check = joblib.load(ENSEMBLE_PKL)\n",
    "\n",
    "_examples = [\n",
    "    \"Lovely ugly top, but too tight\",\n",
    "    \"This dress is beautiful and I love it!\",\n",
    "    \"Cheap fabric and very uncomfortable, runs small\",\n",
    "    \"Perfect fit, high quality material.\",\n",
    "]\n",
    "print(\"\\nSanity check (raw -> adjusted -> label):\")\n",
    "for t in _examples:\n",
    "    lab, p_adj, p_raw = predict_like_api(_model_check, t)\n",
    "    print(f\"- {t}\\n  raw={p_raw:.3f}  adj={p_adj:.3f}  thr={THRESHOLD:.2f}  → {lab}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
